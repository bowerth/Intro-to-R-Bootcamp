pew <- read_excel("data/PEW Middle Class Data.xlsx",
sheet = "3. Median HH income, metro",
skip = 5)
View(pew)
url <- "https://www.data.gov/media/federal-agency-participation.csv"
data_gov <- read.csv(url, stringsAsFactors = FALSE)
read.csv("https://www.data.gov/media/federal-agency-participation.csv", stringsAsFactors = FALSE)
View(data_gov)
str(data_gov)
View(pew)
View(facebook)
str(facebook)
facebook  <- read.delim("data/facebook.tsv", stringsAsFactors = FALSE)
str(facebook)
unique(facebook$gender)
read_excel("http://www.huduser.org/portal/datasets/fmr/fmr2015f/FY2015F_4050_Final.xls")
read_excel("~http://www.huduser.org/portal/datasets/fmr/fmr2015f/FY2015F_4050_Final.xls")
?read_excel
library(gdata)
url <- "http://www.huduser.org/portal/datasets/fmr/fmr2015f/FY2015F_4050_Final.xls"
rents <- read.xls(url)
View(data_gov)
View(rents)
url <- "https://bradleyboehmke.github.io/public/data/reddit.csv"
read.csv(url)
reddit <- read.csv(url)
View(reddit)
rm(list = ls())
mtcars
data(mtcars)
str(mtcars)
View(mtcars)
View(mtcars)
class(mtcars)
mode(mtcars)
length(mtcars)
dim(mtcars)
length(mtcars)
ncol(mtcars)
nrow(mtcars)
names(mtcars)
head(mtcars)
tail(mtcars)
1:10
c(2, 10, 4)
?seq
seq(0, 30, by = 3)
seq(from = 0, to = 30, by = 3)
seq(from = 0, to = 30, length.out = 5)
seq(from = 1, to = 30, length.out = 5)
seq(from = 0, to = 30, length.out = 5)
seq(from = 0, to = 30, length.out = 4)
seq(from = 0, to = 30, length.out = 6)
1:4
rep(1:4, times = 2)
rep(1:4, each = 2)
runif(n = 100, min = 0, max = 10)
hist(runif(n = 100, min = 0, max = 10))
hist(runif(n = 1000, min = 0, max = 10))
hist(runif(n = 10000, min = 0, max = 10))
hist(runif(n = 100000, min = 0, max = 10))
rnorm(n = 100, mean = 10, sd = 2)
hist(rnorm(n = 100, mean = 10, sd = 2))
hist(rnorm(n = 1000, mean = 10, sd = 2))
hist(rnorm(n = 10000, mean = 10, sd = 2))
hist(rnorm(n = 100000, mean = 10, sd = 2))
hist(rnorm(n = 100000, mean = 10, sd = 2))
hist(rnorm(n = 1000000, mean = 10, sd = 2))
hist(rpois(1000, lambda = 4))
hist(rpois(10000, lambda = 4))
rm(mtcars)
facebook <- read.delim("data/facebook.tsv")
facebook$likes
names(facebook)
likes <- facebook$likes
View(facebook)
class(likes)
mode(likes)
str(likes)
length(likes)
dim(likes)
head(likes)
tail(likes, 20)
likes[1]
likes[1:10]
likes[c(10, 13512, 3778, 59131)]
subset_vector <- c(10, 13512, 3778, 59131)
subset_vector
likes[subset_vector]
likes >= 5000
viral <- likes >= 5000
viral
dull <- likes == 0
normal <- likes > 0 & likes < 5000
know_somebody <- likes != 0
dull
know_somebody
normal
str(dull)
sum(viral)
sum(viral) / length(likes)
length(likes)
sum(viral) / length(likes)
sum(viral) / length(likes) *100
which(viral)
likes[which(viral)]
likes[likes > 15000]
which(likes > 15000)
likes[98930]
popular <- likes[likes >= 5000]
lonely <- likes[likes == 0]
summary(likes)
mean(likes)
summary(likes)
mean(likes, na.rm = TRUE)
median(likes, na.rm = T)
sd(likes)
range(likes)
min(likes)
max(likes)
?matrix()
c(1, 2.5, 4.5)
1:10
1.5:10.5
class(1:10)
class(1.5:10.5)
?numeric
mtcars
str(mtcars)
mtcars$cyl
class(mtcars$cyl)
dbl_var <- c(1, 2.5, 4.5)
int_var <- c(1L, 6L, 10L)
class(dbl_var)
class(int_var)
?numeric
is.double(dbl_var)
x <- c(4, 4, 9, 12)
y <- c(4, 4, 9, 12.00000008)
z <- c(4, 4, 9, 12)
x
y
z
x == y
options(digits = 7)
z
y
options(digits = 10)
y
options(digits = 7)
x == y
identical(x, y)
identical(x, z)
all.equal(x, y)         # all.equal assess near equality
x <- c(1, 1.35, 1.7, 2.05, 2.4, 2.75)
round(x)
round(x, digits = 1)
ceiling(x)
floor(x)
rm(list = ls())
gender <- c("male", "female", "female")
age.range <- c("18-24", "25-34", "35-44", "45-54", "55-64", "65 or Above", "Under 18")
levels <- c("Under 18", "18-24", "25-34", "35-44", "45-54", "55-64", "65 or Above")
class(gender)
factor(gender)
factor(gender)
2+2
source('~/.active-rstudio-document')
getwd()
Inf
1 / 0
Inf - Inf
NA
x <- 3                  # assign 3 to x
x
X
D <- 1000
K <- 5
h <- .25
Q <- sqrt((2 * D * K) / h)
Q
rm(D)
rm(x,Q)
rm(list = ls())         # remove all objects
1:10
-10:1
c(9, 5, 6, 13)
y <- c(9, 5, 6, 13)
y
x <- c(1, 3, 4)
y <- c(1, 2, 4)
x + y
x * y
x > y
long <- 1:10
short <- 1:5
long
short
long + short
long * 16
short <- 1:4
long + short
library("class", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
detach("package:class", unload=TRUE)
library(dplyr)                                  # activate package
help(package = "dplyr")                         # provides details regarding package
vignette(package = "dplyr")                     # list vignettes available for a package
vignette("introduction", package = "dplyr")     # view specific vignette
data()
rivers
mtcars
mtcars
rm(list = ls())
data("mtcars")
mtcars
View(mtcars)
data(package = "ggplot2")
?economics
?mtcars
?iris
read.table()
?read.table()
read.csv("data/mydata.csv")
read.delim("data/mydata.txt")
mydata <- read.delim("data/mydata.tsv")
mydata
View(mydata)
facebook <- read.delim("data/facebook.tsv")
View(facebook)
rm(mtcars, mydata)
library(readxl)
url <- "https://www.data.gov/media/federal-agency-participation.csv"
data_gov <- read.csv(url, stringsAsFactors = FALSE)
View(data_gov)
data_gov <- read.csv(url)
str(data_gov)
library(gdata)
url <- "http://www.huduser.org/portal/datasets/fmr/fmr2015f/FY2015F_4050_Final.xls"
rents <- read.xls(url)
View(rents)
?read.xls()
seq(0, 30, by = 3)
seq(0, 30, length.out = 6)
rep(1:4, times = 2)
rep(1:4, each = 2)
runif(n = 100, min = 0, max = 10)
rnorm(n = 100, mean = 10, sd = 2)
runif(n = 100, min = 0, max = 10)
rnorm(n = 100, mean = 10, sd = 2)
hist(rnorm(n = 100, mean = 10, sd = 2))
hist(rnorm(n = 1000, mean = 10, sd = 2))
hist(rnorm(n = 10000, mean = 10, sd = 2))
hist(rnorm(n = 100000, mean = 10, sd = 2))
rm(list = ls())
facebook <- read.delim("data/facebook.tsv")
View(facebook)
likes <- facebook$likes
length(likes)
subset_vector <- c(10, 13512, 3778, 59131)
subset_vector
likes[subset_vector]
subset_vector <- c(10, 13512, 3778, 59131, 99004)
likes[subset_vector]
x <- likes[subset_vector]
x
viral <- likes >= 5000
rm(x)
viral
dull <- likes == 0
normal <- likes > 0 & likes < 5000
know_somebody <- likes != 0
sum(viral)
length(likes)
sum(viral)
sum(viral) / length(likes)
which(viral)
popular <- likes[likes >= 5000]
lonely <- likes[likes == 0]
likes <- c(likes, NA)
tail(likes)
mean(likes)
mean(likes, na.rm = TRUE)
median(likes, na.rm = T)
?which.max()
for(i in 5){
x <- rpois(5, lambda = 5)
print(x)
}
for(i in 1:5){
x <- rpois(5, lambda = i)
print(x)
}
for(i in 1:5){
x <- rpois(1, lambda = i)
print(x)
}
x <- vector(mode = "numeric", length = 5)
for(i in 1:5){
x <- rpois(1, lambda = i)
print(x)
}
x
for(i in 1:5){
x <- rpois(5, lambda = i)
print(x)
}
for (year in 2010:2015){
print(paste("The year is", year))
}
for (i in 2010:2015){
print(paste("The year is", i))
}
for (i in 2010:2015){
paste("The year is", i)
}
for (i in 2010:2015){
output <- paste("The year is", i)
print(output)
}
for (i in 1:10){
sqrt(i)
}
for (i in 1:10){
output <- sqrt(i)
print(output)
}
?for
?`for`
for (i in 2010:2016){
year <- paste("The year is", i)
print(year)
}
output <- NA
str(output)
x <- NA
for (i in 2010:2016){
year <- paste("The year is", i)
output <- c(x, year)
print(ouput)
}
x <- NA
for (i in 2010:2016){
year <- paste("The year is", i)
output <- c(x, year)
print(output)
}
for (i in 1:10){
output <- sqrt(i)
print(output)
}
for (i in 1:10){
output <- sqrt(i)
result <- c(x, output)
}
for (i in 1:10){
output <- sqrt(i)
result <- c(x, output)
print(result)
}
for (i in 1:10){
output <- sqrt(i)
x[i] <- output
print(x)
}
x <- NULL
append(x, i)
x <- NULL
for (i in 1:10){
output <- sqrt(i)
x <- append(x, output)
print(x)
}
for (i in 1:10){
output <- sqrt(i)
x <- append(x, output)
}
x
x <- vector(mode = "character", length = 10)
x <- vector(mode = "numeric", length = 10)
for (i in 1:10){
output <- sqrt(i)
x <- append(x, output)
}
x <- vector(mode = "numeric", length = 10)
for (i in 1:10){
output <- sqrt(i)
x[i] <- output
}
x
x <- NULL
for (i in 1:10){
output <- sqrt(i)
x <- append(x, output)
}
x
sqrt(1:10)
for (i in 2010:2016){
output <- paste("The year is", i)
print(output)
}
for (i in 2010:2016){
output <- cat("The year is", i)
print(output)
}
for (i in 2010:2016){
output <- paste("The year is", i)
print(output)
}
x <- NULL
for (i in 1:10){
output <- sqrt(i)
x <- append(x, output)
}
x
x <- NULL
for (i in 1:10){
output <- paste("The year is", i)
x <- append(x, output)
}
x
x <- NULL
for (i in 2010:2016){
output <- paste("The year is", i)
x <- append(x, output)
}
x
for (i in 2010:2016){
output <- paste("The year is", i)
x[i] <- output
}
x
x <- vector(mode = "numeric", length = 7)
n <- 0
for (i in 2010:2016){
n <- n + 1
output <- paste("The year is", i)
x[n] <- output
}
x
x <- vector(mode = "numeric", length = 7)
counter <- 1
for (i in 2010:2016){
output <- paste("The year is", i)
x[counter] <- output
counter <- counter + 1
}
x
l1 <- list(item1 = 1:3,
item2 = letters[1:5],
item3 = c(T, F, T, T),
item4 = matrix(1:9, nrow = 3))
l1
l1["item4"]
l1[["item4"]]
l1$item4
facebook <- read.delim("data/facebook.tsv")
facebook[, 2]
facebook[, "age"]
facebook[, "age", drop = FALSE]
facebook[, "age"]
facebook[, "age", drop = FALSE]
head(facebook[, "age", drop = FALSE])
url <- "https://bradleyboehmke.github.io/public/data/reddit.csv"
reddit <- read.csv(url)
names(reddit)
summary(reddit$employment.status)
ibrary(tidyr)
library(dplyr)  # to illustrate pipe function
library(EDAWR)
cases
storms
expenditures <- read.csv("data/expenditures.csv")
expenditures %>% gather(Year, Costs, 2:15)
library(tidyr)
expenditures %>% gather(Year, Costs, 2:15)
View(expenditures)
?unite
?select
library(dplyr)
?select
library(readxl)
facebook <- read.delim("data/facebook.tsv")
reddit <- read.csv("data/reddit.csv")
race <- read.csv("data/race-comparison.csv")
supermarket <- read_excel("data/Supermarket Transactions.xlsx", sheet = "Data")
reddit_ed <- reddit %>%
group_by(education) %>%
tally() %>%
filter(education != "NA") %>%
arrange(n)
par(mar = c(5,15,1,1), las = 1)
barplot(reddit_ed$n, names.arg = reddit_ed$education, horiz = TRUE)
cheese <- reddit %>%
group_by(cheese) %>%
tally() %>%
filter(cheese != "NA") %>%
arrange(n)
dotchart(cheese$n, labels = cheese$cheese, bg = "yellow")
par(mar = c(5, 4, 4, 2))
revenue_by_date <- supermarket %>%
group_by(`Purchase Date`) %>%
summarise(Revenue = sum(Revenue, na.rm = TRUE))
plot(Revenue ~ `Purchase Date`, data = revenue_by_date, type = "l", col = "grey")
lines(lowess(revenue_by_date$Revenue ~ revenue_by_date$`Purchase Date`, f = 1/4), col = "blue")
par(mar = c(5, 10, 4, 2))
boxplot(Revenue ~ `Product Category`, data = supermarket, horizontal = TRUE)
par(mar = c(5, 4, 4, 2))
par(mar = c(5, 4, 4, 2))
par(mar = c(5, 10, 4, 2))
boxplot(Revenue ~ `Product Category`, data = supermarket, horizontal = TRUE)
install.packages("learningCurve")
?learningCurve
